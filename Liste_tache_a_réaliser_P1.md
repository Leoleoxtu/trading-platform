# üìã LISTE DE T√ÇCHES D√âVELOPPEUR
## Syst√®me de Trading Algorithmique IA

**Dur√©e estim√©e totale** : 56 jours (8 semaines) + 4 semaines paper trading
**Pr√©-requis** : Python 3.11+, Docker, Git, Compte Anthropic/OpenAI, Compte IBKR

---

## üîß PHASE 1 : INFRASTRUCTURE DE BASE (Semaine 1 - Jours 1-7)

### JOUR 1-2 : Setup Services Fondamentaux

#### T√¢che 1.1 : Initialiser le Projet
- [ ] Cr√©er repo Git : `git init trading-system`
- [ ] Structure de dossiers :
  ```
  trading-system/
  ‚îú‚îÄ‚îÄ docker-compose.yml
  ‚îú‚îÄ‚îÄ docker-compose.scale.yml
  ‚îú‚îÄ‚îÄ .env.example
  ‚îú‚îÄ‚îÄ requirements.txt
  ‚îú‚îÄ‚îÄ src/
  ‚îú‚îÄ‚îÄ config/
  ‚îú‚îÄ‚îÄ tests/
  ‚îú‚îÄ‚îÄ scripts/
  ‚îî‚îÄ‚îÄ docs/
  ```
- [ ] Cr√©er `.gitignore` (env files, __pycache__, logs, data)
- [ ] Premier commit

#### T√¢che 1.2 : Docker Compose Base
- [ ] Cr√©er `docker-compose.yml` avec services :
  - [ ] Redpanda (Kafka)
  - [ ] MinIO (S3)
  - [ ] PostgreSQL
  - [ ] Redis
  - [ ] Kafka UI
- [ ] Tester d√©marrage : `docker-compose up -d`
- [ ] V√©rifier sant√© : `docker-compose ps`

#### T√¢che 1.3 : Configuration Redpanda
- [ ] Cr√©er topics Kafka :
  ```bash
  rpk topic create events.raw.v1 --partitions 10
  rpk topic create events.normalized.v1 --partitions 10
  rpk topic create events.triaged.v1 --partitions 5
  rpk topic create newscards.v1 --partitions 5
  rpk topic create market.ohlcv.v1 --partitions 20
  rpk topic create signals.final.v1 --partitions 3
  rpk topic create orders.intent.v1 --partitions 2
  rpk topic create orders.executed.v1 --partitions 2
  rpk topic create alerts.priority.v1 --partitions 5
  rpk topic create learning.outcomes.v1 --partitions 1
  ```
- [ ] Tester producer/consumer basique
- [ ] Acc√©der Kafka UI : http://localhost:8080

#### T√¢che 1.4 : Configuration MinIO
- [ ] Cr√©er buckets S3 :
  ```
  raw-events
  newscards-archive
  scenarios-archive
  reports
  backups
  ```
- [ ] Configurer lifecycle policy (r√©tention 30-90 jours)
- [ ] Tester upload/download fichier
- [ ] Acc√©der console : http://localhost:9001

#### T√¢che 1.5 : Configuration PostgreSQL
- [ ] Cr√©er base `trading`
- [ ] Cr√©er user `trader` avec permissions
- [ ] Tester connexion : `psql -h localhost -U trader -d trading`

#### T√¢che 1.6 : Configuration Redis
- [ ] Tester connexion : `redis-cli ping`
- [ ] Configurer maxmemory policy : `allkeys-lru`
- [ ] Tester set/get

---

### JOUR 3-4 : TimescaleDB & Monitoring

#### T√¢che 1.7 : Installation TimescaleDB
- [ ] Installer extension TimescaleDB dans PostgreSQL
- [ ] Cr√©er hypertables :
  ```sql
  CREATE TABLE ohlcv (
    time TIMESTAMPTZ NOT NULL,
    ticker VARCHAR(10),
    open NUMERIC,
    high NUMERIC,
    low NUMERIC,
    close NUMERIC,
    volume BIGINT
  );
  SELECT create_hypertable('ohlcv', 'time');
  ```
- [ ] Cr√©er continuous aggregates (VWAP 1h, 1d)
- [ ] Tester insertion donn√©es

#### T√¢che 1.8 : Sch√©ma Base de Donn√©es
- [ ] Cr√©er tables :
  - [ ] `newscards` (event_id, ticker, type, impact, etc.)
  - [ ] `scenarios` (scenario_id, ticker, version, conditions, etc.)
  - [ ] `positions` (position_id, ticker, entry, current, pnl, etc.)
  - [ ] `orders` (order_id, ticker, action, status, etc.)
  - [ ] `decision_logs` (log_id, input_pack, signal, outcome, etc.)
  - [ ] `agent_performance` (agent_name, calls, latency, cost, etc.)
- [ ] Cr√©er index optimis√©s (ticker, timestamp)
- [ ] Cr√©er script migration : `scripts/db_migration.sql`

#### T√¢che 1.9 : Setup Prometheus + Grafana
- [ ] Ajouter services au docker-compose :
  - [ ] Prometheus
  - [ ] Grafana
- [ ] Cr√©er `config/prometheus.yml` avec scrape configs
- [ ] Acc√©der Grafana : http://localhost:3000 (admin/admin)
- [ ] Ajouter datasource Prometheus

#### T√¢che 1.10 : Dashboards Grafana Initiaux
- [ ] Dashboard "System Health" :
  - [ ] Panels : CPU, RAM, Disk, Network
  - [ ] Panel : Redpanda throughput
  - [ ] Panel : PostgreSQL connections
- [ ] Exporter JSON : `dashboards/grafana/system_health.json`
- [ ] Test alerting : Alert si CPU > 80%

---

### JOUR 5-7 : Environment & Testing Infrastructure

#### T√¢che 1.11 : Configuration Environnement
- [ ] Cr√©er `.env.example` :
  ```
  # Databases
  DATABASE_URL=postgresql://trader:password@localhost:5432/trading
  REDIS_URL=redis://localhost:6379
  
  # Kafka
  KAFKA_BROKERS=localhost:9092
  
  # MinIO
  MINIO_ENDPOINT=localhost:9000
  MINIO_ACCESS_KEY=minioadmin
  MINIO_SECRET_KEY=minioadmin
  
  # AI APIs
  ANTHROPIC_API_KEY=sk-ant-...
  OPENAI_API_KEY=sk-...
  
  # Data APIs
  TWITTER_API_KEY=
  REDDIT_CLIENT_ID=
  NEWSAPI_KEY=
  FINNHUB_KEY=
  
  # Broker
  IB_GATEWAY_HOST=localhost
  IB_GATEWAY_PORT=4002
  ```
- [ ] Copier vers `.env` : `cp .env.example .env`
- [ ] Remplir cl√©s API r√©elles

#### T√¢che 1.12 : Requirements Python
- [ ] Cr√©er `requirements.txt` :
  ```
  # Core
  python-dotenv==1.0.0
  pydantic==2.5.0
  loguru==0.7.2
  
  # Kafka
  aiokafka==0.8.1
  
  # Databases
  psycopg2-binary==2.9.9
  sqlalchemy==2.0.23
  redis==5.0.1
  
  # S3
  boto3==1.34.10
  
  # Data Collection
  feedparser==6.0.10
  tweepy==4.14.0
  praw==7.7.1
  requests==2.31.0
  beautifulsoup4==4.12.2
  playwright==1.40.0
  
  # NLP
  spacy==3.7.2
  transformers==4.36.2
  torch==2.1.2
  sentence-transformers==2.2.2
  langdetect==1.0.9
  
  # AI
  langchain==0.1.0
  langgraph==0.0.20
  anthropic==0.8.1
  openai==1.6.1
  
  # Market Data
  yfinance==0.2.33
  ccxt==4.1.92
  ib-insync==0.9.86
  
  # Analysis
  pandas==2.1.4
  numpy==1.26.2
  ta-lib==0.4.28
  
  # API
  fastapi==0.108.0
  uvicorn==0.25.0
  websockets==12.0
  
  # Testing
  pytest==7.4.3
  pytest-asyncio==0.21.1
  hypothesis==6.92.1
  ```
- [ ] Cr√©er venv : `python -m venv venv`
- [ ] Installer : `pip install -r requirements.txt`

#### T√¢che 1.13 : Structure Code Base
- [ ] Cr√©er arborescence `src/` :
  ```
  src/
  ‚îú‚îÄ‚îÄ __init__.py
  ‚îú‚îÄ‚îÄ ingestion/
  ‚îú‚îÄ‚îÄ preprocessing/
  ‚îú‚îÄ‚îÄ nlp/
  ‚îú‚îÄ‚îÄ agents/
  ‚îú‚îÄ‚îÄ knowledge/
  ‚îú‚îÄ‚îÄ strategy/
  ‚îú‚îÄ‚îÄ execution/
  ‚îú‚îÄ‚îÄ backtesting/
  ‚îú‚îÄ‚îÄ learning/
  ‚îú‚îÄ‚îÄ monitoring/
  ‚îî‚îÄ‚îÄ utils/
  ```
- [ ] Cr√©er `__init__.py` dans chaque dossier

#### T√¢che 1.14 : Framework de Test
- [ ] Cr√©er structure `tests/` :
  ```
  tests/
  ‚îú‚îÄ‚îÄ unit/
  ‚îú‚îÄ‚îÄ integration/
  ‚îú‚îÄ‚îÄ e2e/
  ‚îî‚îÄ‚îÄ fixtures/
  ```
- [ ] Cr√©er `pytest.ini`
- [ ] Test basique : `pytest --version`

---

## üìä PHASE 2 : DATA COLLECTION (Semaine 2 - Jours 8-14)

### JOUR 8-10 : Collectors Core

#### T√¢che 2.1 : Interface Abstraite Collector
- [ ] Cr√©er `src/ingestion/base.py` :
  ```python
  from abc import ABC, abstractmethod
  from dataclasses import dataclass
  
  @dataclass
  class RawEvent:
      source: str
      url: str
      text: str
      timestamp: str
      metadata: dict
  
  class Collector(ABC):
      @abstractmethod
      async def collect(self) -> List[RawEvent]:
          pass
  ```

#### T√¢che 2.2 : RSS Collector
- [ ] Cr√©er `src/ingestion/rss_collector.py`
- [ ] Charger sources depuis `config/rss_sources.yaml`
- [ ] Parser avec feedparser
- [ ] Publier vers `events.raw.v1` (Redpanda)
- [ ] Archiver dans MinIO (`raw-events/rss/`)
- [ ] Test unitaire : 10 feeds mock
- [ ] M√©triques Prometheus (feeds_processed, errors)

#### T√¢che 2.3 : Configuration RSS Sources
- [ ] Cr√©er `config/rss_sources.yaml` :
  ```yaml
  sources:
    - name: Bloomberg
      url: https://www.bloomberg.com/feed/...
      priority: high
      quality: 9
    - name: Reuters
      url: https://www.reutersagency.com/feed/...
      priority: high
      quality: 9
    # ... 50+ sources
  ```
- [ ] Ajouter cat√©gories (tech, finance, macro, etc.)

#### T√¢che 2.4 : Twitter Collector
- [ ] Cr√©er `src/ingestion/twitter_collector.py`
- [ ] Setup Tweepy avec API key
- [ ] Filtres : #stocks, #trading, comptes v√©rif√©s
- [ ] Rate limiting (300 calls/15min)
- [ ] Publier vers Redpanda
- [ ] Test avec API sandbox

#### T√¢che 2.5 : Reddit Collector
- [ ] Cr√©er `src/ingestion/reddit_collector.py`
- [ ] Setup PRAW avec credentials
- [ ] Subreddits : wallstreetbets, stocks, investing
- [ ] Filtrer posts score > 50
- [ ] Polling interval : 2 minutes
- [ ] Test unitaire

#### T√¢che 2.6 : News API Collector
- [ ] Cr√©er `src/ingestion/news_api_collector.py`
- [ ] Int√©gration NewsAPI, Finnhub
- [ ] Rate limiting par provider
- [ ] Retry logic avec backoff
- [ ] Fallback si API down

#### T√¢che 2.7 : Web Scraper
- [ ] Cr√©er `src/ingestion/web_scraper.py`
- [ ] Playwright setup
- [ ] Sites cibles : Seeking Alpha, MarketWatch
- [ ] Respecter robots.txt
- [ ] Rate limiting 1 req/5s
- [ ] Rotating user-agents

---

### JOUR 11-14 : Market Data & Preprocessing

#### T√¢che 2.8 : Market Data Collector
- [ ] Cr√©er `src/ingestion/market_collector.py`
- [ ] yfinance pour donn√©es delayed
- [ ] Polygon.io pour real-time (si API key)
- [ ] Tickers watchlist depuis config
- [ ] Insertion directe TimescaleDB
- [ ] Scheduling : 1 min bars pendant market hours

#### T√¢che 2.9 : Feature Calculator
- [ ] Cr√©er `src/ingestion/features.py`
- [ ] Calculer indicateurs :
  - [ ] VWAP (1h, 1d)
  - [ ] RSI (14 p√©riodes)
  - [ ] MACD (12, 26, 9)
  - [ ] Bollinger Bands
  - [ ] ATR (Average True Range)
- [ ] Stocker dans TimescaleDB
- [ ] Test avec donn√©es mock

#### T√¢che 2.10 : Normalizer
- [ ] Cr√©er `src/preprocessing/normalizer.py`
- [ ] Consumer Kafka : `events.raw.v1`
- [ ] Nettoyage texte :
  - [ ] Strip HTML tags
  - [ ] Normaliser Unicode
  - [ ] Supprimer URLs
- [ ] Timestamp vers UTC
- [ ] D√©duplication (BloomFilter Redis)
- [ ] Publier vers `events.normalized.v1`
- [ ] Tests unitaires

#### T√¢che 2.11 : Triage Stage 1 (D√©terministe)
Objectif du Stage 1

Mettre un entonnoir d√©terministe qui :

ne jette presque rien (objectif 100k/jour, garder les signaux faibles)

attribue un score initial 0‚Äì100, une priority hint et des raisons

route les √©v√©nements vers une voie :

FAST (√† traiter imm√©diatement par NLP Stage 2)

STANDARD (√† traiter normalement)

COLD (√† conserver et traiter plus tard / batch / sampling)

DROP_HARD (spam √©vident uniquement)

Le Stage 1 doit √™tre ultra rapide, stable, configurable, observable.

A) Topics & routing (obligatoire)
A1) Entr√©e

Consommer Kafka depuis events.normalized.v1

Consumer group : triage-stage1-v1

A2) Sorties (recommand√©es)

Cr√©er 3 topics de sortie (et 3 DLQ optionnels si tu veux) :

events.stage1.fast.v1 (6 partitions)

events.stage1.standard.v1 (6 partitions)

events.stage1.cold.v1 (6 partitions)

Optionnel :
4) events.stage1.dropped.v1 (1 partition) uniquement pour audit (pas obligatoire)
5) DLQ unique : events.stage1.dlq.v1 (1 partition)

üëâ But : Stage 2 consommera fast + standard en temps r√©el, et cold en batch.

Mettre √† jour infra/redpanda/init-topics.sh pour cr√©er ces topics (idempotent).

B) Contrat de sortie : stage1_event.v1 (obligatoire)

Cr√©er un sch√©ma strict schemas/stage1_event.v1.json.

Champs minimum requis
Identit√© & tra√ßabilit√©

schema_version = "stage1_event.v1"

event_id (UUID)

triaged_at_utc (UTC)

pipeline_version

source_type, source_name

event_time_utc (si dispo)

canonical_url (nullable)

lang

dedup_key (si dispo depuis normalized)

normalized_text_hash (sha256 du texte normalis√© utilis√©)

Triage stage 1

triage_score_stage1 (0‚Äì100)

triage_bucket enum : FAST|STANDARD|COLD|DROP_HARD

priority_hint enum : P0|P1|P2|P3

triage_reasons : array de tags (liste ferm√©e, faible cardinalit√©)

signals : objet r√©sumant les features cheap calcul√©es, ex :

has_ticker_candidate bool

ticker_candidates_count int

has_numbers bool

has_percent bool

has_money bool

text_length int

keyword_hits array (optionnel mais faible cardinalit√©)

source_reliability number 0..1

source_noise number 0..1

recency_seconds int (now - event_time)

normalized_event : (optionnel) soit tu copies un sous-ensemble minimal (title/text/url), soit tu mets une r√©f√©rence.

Recommand√© v1 : inclure le strict minimum utile √† Stage 2 sans re-fetch (mais attention taille).

Minimum : normalized_text, symbols_candidates (si existant), source_score si existant.

Quality flags

quality_flags array : LOW_TEXT, LANG_UNKNOWN, CLICKBAIT_SUSPECT, SPAM_SUSPECT, etc.

R√®gles :

additionalProperties: false

fournir sample schemas/samples/stage1_event_valid.json

mettre √† jour scripts de validation des sch√©mas.

C) Configuration Stage 1 (obligatoire)

Cr√©er un fichier de config versionn√© (dans repo) :

config/triage_stage1.yaml

Contenu minimum :

strong_keywords (liste) : mots cl√©s ‚Äúmarket-moving‚Äù (earnings, SEC, Fed, merger, bankruptcy, hack, breach, guidance‚Ä¶)

weak_keywords (liste) : signaux faibles (rumor, reported, might, sources say‚Ä¶)

clickbait_keywords (liste) : ‚Äúshocking‚Äù, ‚Äúyou won‚Äôt believe‚Äù, etc (p√©nalit√©)

source_scores :

mapping par source_type puis source_name ‚Üí reliability (0‚Äì1) + noise (0‚Äì1)

ex : rss: { "ft.com": {reliability:0.95, noise:0.1}, ... }

thresholds :

score thresholds pour bucket FAST/STANDARD/COLD

limits :

max text length analys√©

min text length

Doit √™tre simple √† ajuster sans toucher au code (l‚Äôagent doit documenter comment).

D) Logique Stage 1 (obligatoire, orient√©e ‚Äúgarder‚Äù)
D1) Extraction ‚Äúcheap signals‚Äù

√Ä partir du normalized event (texte + metadata) :

text_length

has_numbers / has_percent / has_money (regex)

keyword hits (strong + weak + clickbait)

ticker candidates (from symbols_candidates ou regex l√©g√®re si absent)

recency_seconds (si event_time dispo)

source reliability/noise depuis YAML

duplicate_recent (si dedup_key d√©j√† vu r√©cemment par Stage 1, optionnel)

D2) Score Stage 1 (0‚Äì100) ‚Äî d√©terministe & explicable

Construire un score compos√©, ex (structure, pas de code) :

+0..35 : source_reliability (gros poids)

+0..25 : strong keywords hits (cap)

+0..15 : ticker candidates (plus si plusieurs, mais cap)

+0..10 : numbers/percent/money (cap)

+0..10 : recency (plus r√©cent = plus haut)

-0..20 : p√©nalit√©s source_noise, clickbait, texte trop court, etc.

Chaque composant ajout√©/p√©nalis√© doit aussi ajouter un tag triage_reasons :

HIGH_SOURCE_RELIABILITY

STRONG_KEYWORDS

HAS_TICKER_CANDIDATES

HAS_MONEY_OR_PERCENT

VERY_RECENT

HIGH_SOURCE_NOISE

CLICKBAIT_SUSPECT

SHORT_TEXT
etc.

D3) Buckets (FAST/STANDARD/COLD/DROP_HARD)
Principe (important)

DROP_HARD uniquement pour spam √©vident (rare)

COLD = on garde, mais on traite plus tard

FAST/STANDARD = on traite maintenant

R√®gles recommand√©es :

DROP_HARD si :

texte vide/illisible ou

clickbait + source inconnue + aucune entit√©/ticker + tr√®s court

FAST si :

score >= FAST_THRESHOLD (config)

OU pr√©sence keyword ultra fort (SEC/Fed/earnings/merger/bankruptcy/hack) + source >= moyen

STANDARD si :

score >= STANDARD_THRESHOLD

COLD sinon (par d√©faut)

D4) Priority hint (P0..P3)

P0 pour FAST (top urgent)

P1 pour FAST (moins urgent) / STANDARD haut

P2 pour STANDARD bas

P3 pour COLD (signal faible conserv√©)

E) D√©duplication l√©g√®re (recommand√©e)

Stage 1 peut avoir un dedup minimal pour limiter flood :

stocker r√©cemment vus (dedup_key + TTL) dans :

Redis si dispo (id√©al), sinon SQLite ou fichier persistant (volume)

si duplicate r√©cent : ne pas drop forc√©ment, mais :

baisser score

ou router en COLD

et ajouter reason DUPLICATE_RECENT

F) Service services/preprocessing/triage_stage1 (obligatoire)

Cr√©er un nouveau service dockeris√© :

services/preprocessing/triage_stage1/

Dockerfile

app (consumer kafka + producer kafka)

requirements

expose :

GET /health

GET /metrics

port recommand√© : 8006

Ajout dans infra/docker-compose.yml :

service triage-stage1 dans profile apps

env vars : bootstrap kafka, path config YAML, etc.

Prometheus scrape ajout√© (profil observability)

G) Observabilit√© Prometheus (obligatoire)

Exposer m√©triques stables :

Counters :

triage_stage1_events_consumed_total

triage_stage1_events_routed_total{bucket="FAST|STANDARD|COLD|DROP_HARD"}

triage_stage1_events_failed_total{reason="schema|produce|config|runtime"}

triage_stage1_dedup_hits_total
Histograms :

triage_stage1_processing_duration_seconds

triage_stage1_score_histogram (histogram ou summary)
Gauges :

triage_stage1_last_success_timestamp

H) Grafana panels (obligatoire)

Mettre √† jour dashboard existant ou cr√©er ‚ÄúTriage Health‚Äù avec :

rate routed FAST/STANDARD/COLD/DROP_HARD

p95 latency stage1

score distribution (histogram)

dedup hits rate

drop_hard rate (doit rester faible)

last success age

Provisionner automatiquement.

I) Tests & validation (obligatoire)

Cr√©er un test d‚Äôint√©gration simple :

injecter 8 events normalis√©s vari√©s dans events.normalized.v1 :

fort keyword + source fiable ‚Üí FAST

source moyenne + ticker ‚Üí STANDARD

signal faible (rumor) mais ticker ‚Üí COLD (pas drop)

spam √©vident ‚Üí DROP_HARD

v√©rifier qu‚Äôon retrouve les events dans les bons topics

v√©rifier conformit√© sch√©ma stage1_event.v1

J) Documentation (obligatoire)

Cr√©er docs/triage_stage1.md :

philosophie funnel (on garde, on priorise)

explication score + buckets + priority

comment √©diter config/triage_stage1.yaml

comment surveiller sur Grafana

comment ajuster thresholds pour viser 100k/j

Crit√®res d‚Äôacceptation (DoD)

Le service tourne en local via docker compose --profile apps --profile observability up -d

Il consomme events.normalized.v1 et route correctement vers events.stage1.fast.v1, standard, cold

DROP_HARD reste rare et justifi√© (audit possible)

/metrics expose toutes les m√©triques cl√©s

Dashboard montre la distribution et la latence

Tests d‚Äôint√©gration passent

#### T√¢che 2.12 : Triage Stage 2 (NLP)
But du Stage 2

Transformer un √©v√©nement ‚Äúnormalized‚Äù ou ‚Äústage1‚Äù en un √©v√©nement triaged qui contient :

NER (spaCy) : ORG, PERSON + signaux MONEY / PERCENT (au minimum)

Sentiment finance (FinBERT local) : score + confidence

Un score final 0‚Äì100 + raisons (explainable)

Une priority exploitable (P0..P3)

Un seuil adaptatif selon le r√©gime (march√© + charge pipeline)

Publication vers events.triaged.v1 (et DLQ)

Important : on ne veut pas ‚Äútout jeter‚Äù. M√™me des liens faibles doivent passer, mais avec priorit√© basse et/ou marquage.

A) Entr√©e / sortie
Input topic

Consommer depuis events.stage1.v1 (si Stage 1 existe)
sinon consommer depuis events.normalized.v1.

Consumer group : triage-nlp-v1

Output topics

events.triaged.v1 (6 partitions)

events.triaged.dlq.v1 (1 partition)

Mettre √† jour le script init topics Redpanda pour cr√©er ces topics.

B) Sch√©ma de sortie : triaged_event.v1

Cr√©er un sch√©ma JSON strict schemas/triaged_event.v1.json (contract-first). Champs minimum :

Identit√© & tra√ßabilit√©

schema_version = "triaged_event.v1"

event_id (UUID)

triaged_at_utc

pipeline_version

source_type, source_name

canonical_url (nullable)

lang

dedup_key (si dispo) + normalized_text_hash

R√©sultats NLP

entities : liste d‚Äôobjets {type, text, confidence} (ORG, PERSON, PRODUCT minimum)

money_mentions : bool + (optionnel) liste de montants extraits si simple

percent_mentions : bool + (optionnel) liste de pourcentages extraits

Sentiment (FinBERT)

sentiment.score dans [-1,1]

sentiment.confidence dans [0,1]

sentiment.model="finbert"

Score & d√©cision

triage_score (0‚Äì100)

priority enum P0|P1|P2|P3

triage_reasons : array de tags explicatifs (faible cardinalit√©)

thresholds : objet qui log la valeur de seuil utilis√©e (pour audit)

regime : objet {market_regime, load_regime}

Erreurs / flags

quality_flags : array (ex: LOW_TEXT, LANG_UNKNOWN, NER_EMPTY, FINBERT_LOW_CONF)

DLQ : d√©finir un format DLQ stable contenant event_id, error_type, error_message, failed_stage, et (si possible) l‚Äôinput minimal.

C) NLP Pipeline (Stage 2)
C1) Mod√®les spaCy (NER)

Installer et t√©l√©charger en + fr :

en_core_web_sm

fr_core_news_sm

S√©lectionner le mod√®le selon lang de l‚Äôevent (sinon fallback ‚Äúen‚Äù).

Sorties attendues :

ORG et PERSON au minimum

si le mod√®le fournit MONEY/PERCENT directement : les utiliser

sinon : extraire MONEY/PERCENT via regex l√©g√®re (mais garder cette logique stable/d√©terministe)

C2) FinBERT local (sentiment finance)

Utiliser un mod√®le FinBERT adapt√© au sentiment (local).

D√©finir une strat√©gie performance pour 100k/j :

batch inference (par paquet) pour amortir le co√ªt

limiter la longueur texte (ex: max tokens) et stocker un flag ‚ÄúTEXT_TRUNCATED‚Äù

Sortie :

score : map vers [-1,1]

confidence : probabilit√© du label choisi

C3) R√©solution tickers ‚Äúmieux que regex‚Äù

M√™me si ce service s‚Äôappelle triage, il doit aider √† √©viter les faux positifs :

Entr√©e : symbols_candidates (du normalizer) + ORG entities spaCy

Validation minimale acceptable :

‚Äúwhitelist‚Äù tickers configurable (CSV) OU

table DB (Timescale / Postgres) des tickers connus (si disponible)

Sortie :

tickers: liste {symbol, confidence, method}

D) Scoring 0‚Äì100 (explainable)

Construire un score compos√© de sous-scores (et produire les reasons) :

D1) Sous-scores recommand√©s

Impact keywords (macro/earnings/SEC/merger/bankruptcy/security hack) : +0..30

Source quality (h√©rit√© Stage 1 si dispo) : +0..25

Ticker confidence : +0..20

Entity strength (ORG/PERSON) : +0..10

Sentiment magnitude (|score|) et confiance : +0..15
(mais p√©naliser si confidence faible)

Score final = clamp 0..100.

D2) ‚ÄúReasons‚Äù obligatoires

√Ä chaque boost/p√©nalit√©, ajouter un tag dans triage_reasons (liste ferm√©e) :

HIGH_SOURCE_QUALITY

HAS_VALID_TICKER

KEYWORD_EARNINGS

KEYWORD_REGULATION

KEYWORD_MACRO

KEYWORD_SECURITY

STRONG_SENTIMENT

LOW_SENTIMENT_CONF

SHORT_TEXT

NER_EMPTY
etc.

E) Seuil adaptatif selon r√©gime (march√© + charge)

On ne ‚Äúdrop‚Äù pas agressivement : on adapte surtout la priority et le routing.

E1) Market regime (simple, d√©terministe)

D√©finir 3 r√©gimes :

CALM

NORMAL

STRESS

M√©thode minimale :

utiliser un indicateur externe si d√©j√† dispo (ex: VIX) sinon

calculer un proxy √† partir de SPY/QQQ dans Timescale (vol r√©cente)

en stress : abaisser les seuils pour mettre plus de choses en P0/P1

E2) Load regime (pipeline)

D√©finir 3 √©tats :

LOW_LOAD, NORMAL_LOAD, HIGH_LOAD

D√©clencheurs possibles (choisir au moins 2) :

consumer lag > X

latence p95 > Y

taux d‚Äôerreurs/DLQ > Z

CPU/m√©moire au-dessus d‚Äôun seuil (si m√©triques dispo)

En HIGH_LOAD :

ne pas jeter : d√©grader :

plus d‚Äôevents basculent en P3 (COLD) au lieu de P1/P2

sampling optionnel sur P3 pour Stage 2 si besoin, mais conserver en MinIO/artefacts

E3) D√©finir les seuils

Exemple de logique (√† impl√©menter de fa√ßon document√©e) :

P0 si score >= T0

P1 si score >= T1

P2 si score >= T2

P3 sinon

Avec adaptation :

en STRESS : (T0,T1,T2) diminuent

en HIGH_LOAD : (T0,T1,T2) augmentent l√©g√®rement, mais P3 reste conserv√© (pas drop)

Logguer thresholds et regime dans l‚Äôevent triag√©.

F) Publication & priorit√©

Publier vers events.triaged.v1 :

champ priority dans le JSON

(optionnel) ajouter un header Kafka x-priority=P0..P3 si support√©, mais la v√©rit√© reste le champ JSON.

DLQ :

publier vers events.triaged.dlq.v1 sur toute exception, avec contexte.

G) Observabilit√© (obligatoire)

Exposer /health et /metrics (Prometheus) :

throughput in/out

latence p95 processing

ratio P0/P1/P2/P3

DLQ rate

distribution des scores (histogram)

drift simple :

moyenne mobile sentiment

r√©partition cat√©gories / reasons (faible cardinalit√©)

Mettre √† jour Prometheus scrape + ajouter panels Grafana :

triage_nlp_rate_in, triage_nlp_rate_out

triage_nlp_latency_p95

triage_priority_distribution

triage_dlq_rate

sentiment_mean (drift)

H) Tests & validation (obligatoire)

Cr√©er un test d‚Äôint√©gration qui :

injecte 5 events normalis√©s vari√©s (FR/EN, avec/sans ticker, avec keywords)

v√©rifie qu‚Äôils ressortent dans events.triaged.v1

v√©rifie que :

sch√©ma valid√©

priority attribu√©e

entities pr√©sentes sur au moins 1 event

sentiment pr√©sent + confidence

thresholds/regime pr√©sents

I) Documentation (obligatoire)

Ajouter docs/triage_stage2.md :

scoring expliqu√© + reasons

r√©gime et seuils adaptatifs

comment lire le dashboard

comment ajuster : keywords, seuils, whitelist tickers

Output attendu

Topics + init

Sch√©ma triaged_event.v1

Service Stage 2 (Docker + endpoints health/metrics)

Prometheus/Grafana panels

Tests + docs

Aucun code trading / agents dans cette t√¢che

#### T√¢che 2.13 : Orchestration Collectors
- [ ] Cr√©er `src/ingestion/orchestrator.py`
- [ ] Schedule tous collectors (APScheduler)
- [ ] RSS : 5 min
- [ ] Twitter : 1 min (si stream)
- [ ] Reddit : 2 min
- [ ] Market : 1 min (pendant heures ouverture)
- [ ] Health checks
- [ ] Graceful shutdown

#### T√¢che 2.14 : Tests Integration Data Pipeline
- [ ] Test end-to-end :
  - [ ] Inject event ‚Üí RSS collector
  - [ ] V√©rifier Redpanda (`events.raw.v1`)
  - [ ] V√©rifier MinIO (archive)
  - [ ] V√©rifier Normalizer traite
  - [ ] V√©rifier Triage filtre
- [ ] Mesurer latency totale (< 5s acceptable)

---

## ü§ñ PHASE 3 : AI CORE (Semaine 3-4 - Jours 15-28)

### JOUR 15-18 : Standardizer (NewsCards)

#### T√¢che 3.1 : Multi-Provider Setup
- [ ] Cr√©er `src/agents/providers/` :
  - [ ] `anthropic_provider.py`
  - [ ] `openai_provider.py`
  - [ ] `base_provider.py` (interface)
- [ ] Config `config/ai_providers.yaml` :
  ```yaml
  providers:
    - name: anthropic
      models:
        fast: claude-haiku-4-5-20251001
        medium: claude-sonnet-4-5-20250929
        deep: claude-opus-4-20250514
      weight: 0.6
    - name: openai
      models:
        fast: gpt-4o-mini
        medium: gpt-4o
        deep: o1-preview
      weight: 0.4
  ```

#### T√¢che 3.2 : NewsCard Schema
- [ ] Cr√©er `src/agents/schemas.py` :
  ```python
  from pydantic import BaseModel
  
  class NewsCard(BaseModel):
      event_id: str
      timestamp: str
      entities: List[str]
      tickers: List[str]
      type: str
      impact_direction: str
      impact_strength: float
      time_horizon: str
      novelty: str
      confidence: float
      uncertainties: List[str]
      why_it_matters: List[str]
      invalidated_if: List[str]
      evidence_refs: List[str]
  ```

#### T√¢che 3.3 : Prompt Template NewsCard
- [ ] Cr√©er `src/agents/prompts/newscard_prompt.txt`
- [ ] Variables : {normalized_event}, {context}
- [ ] Output strict JSON
- [ ] Examples few-shot (3-5)

#### T√¢che 3.4 : Standardizer Core
- [ ] Cr√©er `src/agents/standardizer.py`
- [ ] Consumer Kafka : `events.triaged.v1`
- [ ] S√©lection provider/model selon priority
- [ ] Appel LLM avec prompt
- [ ] Parse JSON response
- [ ] Retry si malformed (max 3)
- [ ] Validation schema Pydantic
- [ ] Stockage :
  - [ ] PostgreSQL (metadata)
  - [ ] MinIO (NewsCard compl√®te)
  - [ ] Redis (cache last 100 par ticker)
- [ ] Publier vers `newscards.v1`

#### T√¢che 3.5 : Confidence Calibration
- [ ] Cr√©er `src/agents/calibration.py`
- [ ] Function empirical_calibrate(raw_confidence)
- [ ] Placeholder (lin√©aire) :
  ```python
  def calibrate(conf):
      if conf > 0.9: return conf * 0.75
      elif conf < 0.6: return conf * 1.1
      return conf
  ```
- [ ] Appliquer avant stockage NewsCard

#### T√¢che 3.6 : Tests Standardizer
- [ ] Mock API responses
- [ ] Test 10 events vari√©s
- [ ] Validation JSON structure
- [ ] Test fallback provider si primary fail
- [ ] Test calibration

---

### JOUR 19-21 : Plan Builder

#### T√¢che 3.7 : Catalyst Calendar Integration
- [ ] Cr√©er `src/knowledge/calendar.py`
- [ ] API Trading Economics (gratuit)
- [ ] Earnings dates via yfinance
- [ ] Fed meetings (hardcod√© + API)
- [ ] Stocker dans PostgreSQL (`catalysts` table)
- [ ] Fonction `get_upcoming_catalysts(ticker, days=7)`

#### T√¢che 3.8 : Market Regime Detector
- [ ] Cr√©er `src/strategy/regime_detector.py`
- [ ] Calculer :
  - [ ] VIX actuel
  - [ ] SPY return 5D
  - [ ] SPY volatility 30D
- [ ] Classifier :
  ```python
  if vix > 35: return "FLASH_CRASH"
  elif vix < 12 and vol < 0.005: return "LOW_VOL_GRIND"
  elif vix > 25 and ret_5d < -0.01: return "TRENDING_BEAR"
  elif vix < 15 and ret_5d > 0.01: return "TRENDING_BULL"
  else: return "VOLATILE_RANGE"
  ```
- [ ] Cacher dans Redis (update toutes les 5 min)

#### T√¢che 3.9 : Scenario Schema
- [ ] Cr√©er `src/agents/schemas.py` (ajouter) :
  ```python
  class Scenario(BaseModel):
      scenario_id: str
      ticker: str
      name: str
      version: str
      bias: str  # bullish/bearish/neutral
      probability: float
      entry_conditions: List[str]
      invalidation_triggers: List[str]
      targets: dict
      size_max_pct: float
      time_horizon: str
      reassess_if: List[str]
      reasoning: List[str]
      catalysts_pending: List[dict]
  ```

#### T√¢che 3.10 : Prompt Template Scenario
- [ ] Cr√©er `src/agents/prompts/scenario_prompt.txt`
- [ ] Input : NewsCards 24h, OHLCV 90D, Catalysts, Regime
- [ ] Output : 3 sc√©narios (bullish/neutral/bearish)
- [ ] JSON strict

#### T√¢che 3.11 : Plan Builder Core
- [ ] Cr√©er `src/agents/plan_builder.py`
- [ ] Trigger : Cron 04:00 ET
- [ ] Pour chaque ticker watchlist :
  - [ ] Charger NewsCards depuis 20:00 veille
  - [ ] Charger OHLCV 90D
  - [ ] Charger catalysts proches
  - [ ] Get market regime
  - [ ] Appel LLM (Opus/o1)
  - [ ] Parse scenarios
  - [ ] Stockage PostgreSQL + MinIO
- [ ] G√©n√©ration watchlist dynamique (top 20)

#### T√¢che 3.12 : Scenario Updater
- [ ] Cr√©er `src/agents/scenario_updater.py`
- [ ] Trigger : 11:30, 13:30, 15:30 ET
- [ ] Pour chaque ticker avec position OU sc√©nario actif :
  - [ ] Charger derni√®res 2h donn√©es
  - [ ] Re-run prompt (Sonnet)
  - [ ] Update sc√©narios (version++)
  - [ ] Mark old version superseded
- [ ] M√©triques (scenarios_updated, cost)

#### T√¢che 3.13 : Tests Plan Builder
- [ ] Mock NewsCards (10 pour AAPL)
- [ ] Mock OHLCV historique
- [ ] Test g√©n√©ration 3 sc√©narios
- [ ] Validation structure JSON
- [ ] Test catalyst injection

---

### JOUR 22-28 : Decision Engine (LangGraph)

#### T√¢che 3.14 : LangGraph Workflow Setup
- [ ] Cr√©er `src/agents/decision_engine.py`
- [ ] D√©finir StateGraph :
  ```python
  from langgraph.graph import StateGraph
  
  workflow = StateGraph(DecisionState)
  workflow.add_node("load_context", load_context_node)
  workflow.add_node("match_scenarios", match_scenarios_node)
  workflow.add_node("evaluate_confidence", evaluate_confidence_node)
  workflow.add_node("web_research", web_research_node)
  workflow.add_node("decide", decide_node)
  workflow.add_node("risk_soft", risk_soft_node)
  
  workflow.set_entry_point("load_context")
  workflow.add_edge("load_context", "match_scenarios")
  workflow.add_conditional_edges(
      "evaluate_confidence",
      route_by_confidence,
      {"low": "web_research", "medium": "decide", "high": "decide"}
  )
  ```

#### T√¢che 3.15 : Node: Load Context
- [ ] Fonction `load_context_node(state)`
- [ ] Charger :
  - [ ] NewsCards (fen√™tre 2h)
  - [ ] Scenarios actifs
  - [ ] OHLCV (1D + 5D)
  - [ ] Positions actuelles
  - [ ] Risk limits
- [ ] Return updated state

#### T√¢che 3.16 : Node: Match Scenarios
- [ ] Fonction `match_scenarios_node(state)`
- [ ] Pour chaque sc√©nario :
  - [ ] V√©rifier entry_conditions
  - [ ] Calculer match_score (0-100)
- [ ] Garder top 2 sc√©narios
- [ ] Update state

#### T√¢che 3.17 : Node: Evaluate Confidence
- [ ] Fonction `evaluate_confidence_node(state)`
- [ ] Agr√®ge :
  - [ ] match_score scenarios
  - [ ] NewsCard.confidence (calibr√©e)
  - [ ] Technical confirmation (RSI, MACD)
- [ ] Output : confidence finale (0-1)

#### T√¢che 3.18 : Node: Web Research (optionnel)
- [ ] Cr√©er `src/agents/web_researcher.py`
- [ ] Tavily API ou Perplexity
- [ ] Budget : 20 calls/jour max
- [ ] Timeout : 15s
- [ ] Return : sources + extraits

#### T√¢che 3.19 : Node: Decide
- [ ] Fonction `decide_node(state)`
- [ ] Prompt LLM (Sonnet/GPT-4o) :
  - [ ] Input : full context
  - [ ] Output : Signal JSON
    ```json
    {
      "action": "BUY|SELL|HOLD",
      "confidence": 0.78,
      "reasoning": [...],
      "plan": {
        "order_type": "LIMIT",
        "quantity": 10,
        "limit_price": 185.5,
        "stop_loss": 182.0,
        "take_profit": [189.0, 192.0]
      }
    }
    ```

#### T√¢che 3.20 : Node: Risk Soft Gate
- [ ] Fonction `risk_soft_node(state)`
- [ ] V√©rifier overrides IA :
  - [ ] Stop ajustement dans ¬±20%
  - [ ] Hold malgr√© drawdown < 2.5%
- [ ] Logger tous overrides
- [ ] Return approved/rejected

#### T√¢che 3.21 : Decision Engine Orchestration
- [ ] Consumer Kafka : `newscards.v1` (pour positions held)
- [ ] Consumer Kafka : `alerts.priority.v1` (pour r√©√©valuations)
- [ ] Pour chaque trigger :
  - [ ] Run LangGraph workflow
  - [ ] Publier Signal vers `signals.final.v1`
- [ ] M√©triques (decisions/hour, latency, cost)

#### T√¢che 3.22 : Tests Decision Engine
- [ ] Mock context complet
- [ ] Test workflow end-to-end
- [ ] Test routing confidence (low/medium/high)
- [ ] Test web research trigger
- [ ] Validation Signal output

---

## üõ°Ô∏è PHASE 4 : RISK & EXECUTION (Semaine 5 - Jours 29-35)

### JOUR 29-31 : Risk Management

#### T√¢che 4.1 : Risk Gate Hard
- [ ] Cr√©er `src/execution/risk_gate.py`
- [ ] Config `config/risk_limits.yaml` :
  ```yaml
  hard_limits:
    max_position_pct: 0.10
    max_daily_loss_pct: 0.03
    max_drawdown_pct: 0.15
    max_open_positions: 5
    max_trades_per_day: 10
    stop_loss_required: true
    halt_if_vix_above: 40
  ```
- [ ] Function `check_hard_limits(signal, portfolio)` :
  - [ ] Return GO | REJECT + reason
  - [ ] Aucune exception possible
  - [ ] Log violations

#### T√¢che 4.2 : Correlation Guardian
- [ ] Cr√©er `src/strategy/correlation_guardian.py`
- [ ] Calculer matrice corr√©lation rolling 30D
- [ ] Stocker dans Redis (update 1x/heure)
- [ ] Function `check_correlation(new_ticker, held_positions)` :
  - [ ] Si corr > 0.7 : REDUCE size 50%
  - [ ] Si 2+ positions corr√©l√©es : REJECT
- [ ] Alert si "correlation creep" d√©tect√©

#### T√¢che 4.3 : Pre-Flight Check
- [ ] Cr√©er `src/execution/preflight.py`
- [ ] Checks :
  ```python
  def preflight_check(signal):
      checks = []
      
      # Catalyst imminent ?
      if catalyst_within_minutes(signal.ticker, 30):
          return ABORT
      
      # Correlation OK ?
      corr = max_correlation(signal.ticker)
      if corr > 0.7:
          signal.quantity *= 0.5
          checks.append(WARN("Correlation high"))
      
      # R√©gime OK ?
      if get_regime() == "FLASH_CRASH":
          return ABORT
      
      # Liquidit√© OK ?
      if get_volume(signal.ticker) < 500_000:
          return ABORT
      
      # Spread OK ?
      if get_spread(signal.ticker) > 0.005:
          return WAIT
      
      return GO(checks)
  ```

#### T√¢che 4.4 : Tests Risk Management
- [ ] Test hard limits (position size, daily loss, etc.)
- [ ] Test correlation checks
- [ ] Test preflight avec scenarios vari√©s
- [ ] Valider aucun bypass possible

---

### JOUR 32-35 : Execution Layer

#### T√¢che 4.5 : Interactive Brokers Setup
- [ ] Installer IB Gateway ou TWS
- [ ] Configuration paper trading :
  - [ ] Port 4002
  - [ ] Enable API connections
  - [ ] Client ID : 1
- [ ] Test connexion : `ib-insync`

#### T√¢che 4.6 : Execution Adapter
- [ ] Cr√©er `src/execution/ibkr_adapter.py`
- [ ] Consumer Kafka : `signals.final.v1`
- [ ] Pour chaque signal :
  - [ ] Re-check