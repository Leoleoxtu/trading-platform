# R√©solution Probl√®me : Dashboard Grafana "NO DATA"

## üìã Contexte

Lors de la mise en place de l'observabilit√© pour le service **Triage Stage 1**, les dashboards Grafana affichaient syst√©matiquement "NO DATA" malgr√© :
- Le service fonctionnant correctement
- Prometheus scrapant les m√©triques
- Les m√©triques visibles via `curl http://localhost:8006/metrics`

## üîç Probl√®mes Identifi√©s

### Probl√®me #1 : Configuration de Provisioning Incorrecte

**Sympt√¥me** :
```bash
curl -s "http://admin:admin@localhost:3001/api/search?type=dash-db"
# Retournait une liste vide ou dashboards manquants
```

**Cause** :
Le fichier `dashboards.yml` √©tait plac√© √† la racine du dossier `provisioning/` au lieu de `provisioning/dashboards/`.

**Structure incorrecte** :
```
infra/observability/grafana/
‚îú‚îÄ‚îÄ provisioning/
‚îÇ   ‚îú‚îÄ‚îÄ dashboards.yml          ‚ùå INCORRECT
‚îÇ   ‚îî‚îÄ‚îÄ datasources/
‚îÇ       ‚îî‚îÄ‚îÄ datasource.yml
‚îî‚îÄ‚îÄ dashboards/
    ‚îî‚îÄ‚îÄ triage_stage1.json
```

**Structure correcte** :
```
infra/observability/grafana/
‚îú‚îÄ‚îÄ provisioning/
‚îÇ   ‚îú‚îÄ‚îÄ dashboards/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboards.yml      ‚úÖ CORRECT
‚îÇ   ‚îî‚îÄ‚îÄ datasources/
‚îÇ       ‚îî‚îÄ‚îÄ datasource.yml
‚îî‚îÄ‚îÄ dashboards/
    ‚îî‚îÄ‚îÄ triage_stage1.json
```

**Solution appliqu√©e** :
```bash
mkdir -p /home/leox7/trading-platform/infra/observability/grafana/provisioning/dashboards
mv /home/leox7/trading-platform/infra/observability/grafana/provisioning/dashboards.yml \
   /home/leox7/trading-platform/infra/observability/grafana/provisioning/dashboards/
```

**Logs Grafana apr√®s correction** :
```
logger=provisioning.dashboard msg="starting to provision dashboards"
logger=provisioning.dashboard msg="finished to provision dashboards"
```

---

### Probl√®me #2 : Codec Snappy Manquant

**Sympt√¥me** :
```bash
docker compose logs triage-stage1
# ERROR: UnsupportedCodecError: Libraries for snappy compression codec not found
```

Le service crashait en boucle au d√©marrage, emp√™chant toute consommation d'√©v√©nements Kafka.

**Cause** :
Redpanda utilise la compression Snappy par d√©faut, mais les biblioth√®ques n√©cessaires n'√©taient pas install√©es dans le conteneur Docker.

**Solution appliqu√©e** :

1. **Dockerfile** - Ajout de la d√©pendance syst√®me :
```dockerfile
# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    libsnappy-dev \    # ‚úÖ Ajout√©
    curl \             # ‚úÖ Ajout√© pour healthcheck
    && rm -rf /var/lib/apt/lists/*
```

2. **requirements.txt** - Ajout du binding Python :
```txt
# Triage Stage 1 Service Requirements
aiokafka>=0.9.0
kafka-python-ng>=2.2.0
python-snappy>=0.7.0    # ‚úÖ Ajout√©
redis==5.0.1
pyyaml==6.0.1
loguru==0.7.2
aiohttp==3.9.1
prometheus-client==0.19.0
```

3. **Rebuild du conteneur** :
```bash
cd /home/leox7/trading-platform/infra
docker compose build triage-stage1 --no-cache
docker compose up -d triage-stage1
```

**Validation** :
```bash
docker compose logs triage-stage1 --tail 20
# ‚úÖ INFO: Triage Stage 1 started
# ‚úÖ INFO: Started consuming from events.normalized.v1
# ‚úÖ DEBUG: Triaged event: score=75, bucket=FAST
```

---

### Probl√®me #3 : UID de Datasource Incorrect

**Sympt√¥me** :
Dashboard visible dans Grafana mais tous les panels affichent "NO DATA" m√™me avec des √©v√©nements trait√©s.

**Cause** :
Le dashboard JSON utilisait un UID de datasource g√©n√©rique `"prometheus"` au lieu de l'UID r√©el g√©n√©r√© par Grafana.

**Diagnostic** :
```bash
# V√©rifier l'UID r√©el de la datasource
curl -s "http://admin:admin@localhost:3001/api/datasources" | \
  python3 -c "import sys,json; ds=[d for d in json.load(sys.stdin) if d['type']=='prometheus'][0]; print(f'Real UID: {ds[\"uid\"]}')"
# Real UID: PBFA97CFB590B2093

# V√©rifier l'UID dans le dashboard
grep -o '"uid": "[^"]*"' triage_stage1.json | head -5
# "uid": "prometheus"    ‚ùå INCORRECT
```

**Solution appliqu√©e** :
```bash
cd /home/leox7/trading-platform/infra/observability/grafana/dashboards

# Backup
cp triage_stage1.json triage_stage1.json.bak

# Remplacement global
sed -i 's/"uid": "prometheus"/"uid": "PBFA97CFB590B2093"/g' triage_stage1.json

# V√©rification
grep -c 'PBFA97CFB590B2093' triage_stage1.json
# 35 occurrences remplac√©es ‚úÖ
```

**Red√©marrage de Grafana** :
```bash
docker compose restart grafana
sleep 15
```

---

### Probl√®me #4 : Variable d'Environnement Kafka Incorrecte

**Sympt√¥me** :
```
ERROR: KafkaConnectionError: Unable to bootstrap from [('localhost', 9092)]
```

**Cause** :
Le code utilisait `os.getenv('KAFKA_BROKERS')` mais Docker Compose d√©finissait `KAFKA_BOOTSTRAP_SERVERS`.

**Solution appliqu√©e** :

**Fichier** : `services/preprocessing/triage_stage1/app.py`
```python
# Avant ‚ùå
kafka_bootstrap_servers=os.getenv('KAFKA_BROKERS', 'localhost:9092'),

# Apr√®s ‚úÖ
kafka_bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092'),
```

---

### Probl√®me #5 : Conflit de Ports

**Sympt√¥me** :
```
Error: Bind for 0.0.0.0:8006 failed: port is already allocated
```

**Cause** :
Le service `feature-store` utilisait d√©j√† le port 8006 (`8006:8000`) et `triage-stage1` tentait aussi d'utiliser le m√™me port (`8006:8006`).

**Solution appliqu√©e** :

**Fichier** : `infra/docker-compose.yml`
```yaml
# feature-store - Chang√© le port externe
ports:
  - "8007:8000"    # ‚úÖ Avant: 8006:8000

# triage-stage1 - Garde le port 8006
ports:
  - "8006:8006"    # ‚úÖ OK
```

---

## ‚úÖ Validation Finale

### 1. Service Healthy
```bash
curl -s http://localhost:8006/health
# {"status": "ok", "running": true, "timestamp": "2025-12-31T09:27:00Z"}
```

### 2. M√©triques Expos√©es
```bash
curl -s http://localhost:8006/metrics | grep "triage_stage1_events" | head -5
# triage_stage1_events_consumed_total 38.0
# triage_stage1_events_routed_total{bucket="FAST"} 2.0
# triage_stage1_events_routed_total{bucket="STANDARD"} 33.0
# triage_stage1_events_routed_total{bucket="COLD"} 3.0
```

### 3. Prometheus Target UP
```bash
curl -s "http://localhost:9090/api/v1/targets" | \
  python3 -c "import sys,json; t=[x for x in json.load(sys.stdin)['data']['activeTargets'] if x['labels'].get('job')=='triage-stage1'][0]; print(f'Health: {t[\"health\"].upper()}')"
# Health: UP
```

### 4. Dashboards Visibles
```bash
curl -s "http://admin:admin@localhost:3001/api/search?type=dash-db" | \
  python3 -c "import sys,json; print(len(json.load(sys.stdin)), 'dashboards trouv√©s')"
# 5 dashboards trouv√©s
```

### 5. Donn√©es dans Grafana
```bash
# Test d'une requ√™te PromQL via Grafana
curl -s "http://admin:admin@localhost:3001/api/datasources/proxy/uid/PBFA97CFB590B2093/api/v1/query?query=triage_stage1_events_consumed_total" | \
  python3 -c "import sys,json; r=json.load(sys.stdin)['data']['result']; print(f'Value: {r[0][\"value\"][1]} events')"
# Value: 38 events
```

### 6. Dashboard Accessible
URL : http://localhost:3001/d/triage-stage1/triage-stage-1?orgId=1&refresh=10s&from=now-15m&to=now

Login : `admin` / `admin`

---

## üìä R√©sum√© des Changements

| Fichier | Modification | Raison |
|---------|--------------|--------|
| `infra/observability/grafana/provisioning/dashboards.yml` | D√©plac√© dans `provisioning/dashboards/` | Structure de provisioning Grafana |
| `services/preprocessing/triage_stage1/Dockerfile` | Ajout `libsnappy-dev` + `curl` | Support compression Kafka |
| `services/preprocessing/triage_stage1/requirements.txt` | Ajout `python-snappy>=0.7.0` | Binding Python pour Snappy |
| `services/preprocessing/triage_stage1/app.py` | `KAFKA_BROKERS` ‚Üí `KAFKA_BOOTSTRAP_SERVERS` | Alignement avec docker-compose |
| `infra/docker-compose.yml` | Port feature-store : `8006` ‚Üí `8007` | R√©solution conflit de ports |
| `infra/observability/grafana/dashboards/triage_stage1.json` | UID datasource : `prometheus` ‚Üí `PBFA97CFB590B2093` | UID r√©el de Prometheus |

---

## üöÄ Commandes de Test

### G√©n√©rer un flux continu d'√©v√©nements
```bash
./scripts/inject_test_events.sh
```

### V√©rifier les m√©triques en temps r√©el
```bash
watch -n 2 'curl -s http://localhost:8006/metrics | grep consumed_total'
```

### Tester les requ√™tes PromQL
```bash
# Ingest rate
curl -s "http://localhost:9090/api/v1/query?query=rate(triage_stage1_events_consumed_total[5m])"

# Distribution par bucket
curl -s "http://localhost:9090/api/v1/query?query=sum%20by(bucket)%20(triage_stage1_events_routed_total)"
```

---

## üìö Le√ßons Apprises

1. **Structure de provisioning Grafana** : Les fichiers de configuration doivent √™tre dans des sous-dossiers sp√©cifiques (`dashboards/`, `datasources/`, `notifiers/`)

2. **D√©pendances Kafka** : Toujours v√©rifier les codecs de compression support√©s par les clients Kafka

3. **UID Datasource** : Les UIDs de datasource sont g√©n√©r√©s dynamiquement par Grafana et doivent √™tre r√©f√©renc√©s correctement dans les dashboards

4. **Variables d'environnement** : Maintenir la coh√©rence entre les noms de variables dans le code et docker-compose

5. **Allocation de ports** : Documenter clairement les ports utilis√©s par chaque service pour √©viter les conflits

---

## üîó R√©f√©rences

- Documentation Grafana Provisioning : https://grafana.com/docs/grafana/latest/administration/provisioning/
- AIOKafka Compression : https://aiokafka.readthedocs.io/en/stable/
- Prometheus Query API : https://prometheus.io/docs/prometheus/latest/querying/api/
- Guide d'op√©rations : [docs/90_operations.md](90_operations.md)
- Validation compl√®te : [docs/TRIAGE_STAGE1_OBSERVABILITY_VALIDATION.md](TRIAGE_STAGE1_OBSERVABILITY_VALIDATION.md)
